{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic Machine Learning - Comparison Between Linear Regression, Random Forest and XGB comparison\n\n## Introduction & Problem Statement\n\nAs many of you probably know, Titanic was an ocean liner that sank in the North Atlantic Ocean due to crashing into an Iceberg. The ship made stops in Cherboug, Queensland and Southampton before it was supposed to travel to New York. The passengers are split into 3 different classes. A total of 2224 passengers boarded the ship but only 710 survived the crash, making the survival rate only 32%. The goal of this project was to accurately predict which passengers survive based on given data. \n\n\n## Summary\n\nIn this kernel, three different machine learning techniques will be employed to predict whether a passenger survives the titanic crash. First, plots were created to better understand the data and then the missing data was filled. Various features were also engineered to improve the performance ofthe models. The most accurate model was the XGB followed by the decision tree and linear regression. \n\n\n# Contents\n\n* [Preprocessing](#Preprocessing)\n    - [Exploratory Data Analysis](#Plots)\n    - [Filling in Missing Data](#Missing_data)\n    - [Feature Engineering and Scaling](#feature_eng)\n* [Machine Learning](#Ml_models)\n    - [Logistic Regression](#log_reg)\n    - [Random Forest](#Rand_Forest)\n    - [XGB](#XGB)\n* [Conclusion](#conclusion)"},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing <a id=\"Preprocessing\"></a>\n\n## Import Libraries and Data <a id=\"ImportLibrariesandData\"></a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\n\ntrain_raw=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ny_train=train_raw['Survived']\ntest_raw=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndfs=[train_raw, test_raw]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis <a id=\"Plots\"></a>\n\nFirst need to understand data better to be able to:\n1. Fill in missing data\n1. Perform feature engineering\n\n## **Preview Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_raw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding Missing Data\n\nSee which features are missing data\n- Age and cabin have a large number of missing data ==> Drop Cabin, fill in age data using some sort of metric\n- Embarked and Fare have a few missing data ==> Impute these values using mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Data',train_raw.isnull().sum(),' ',sep='\\n\\n')\nprint('Test Data',test_raw.isnull().sum(),sep='\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Correlation Plot**\n\n- Overall none of the combinations show strong relationships. <br>\n- The combinations with highest correlations are Fare and Passanger Class (-0.55), # of Siblings/Spouses and # of Parent/Children (0.41) and Passanger Class and Age (-0.37). <br>\n- The highest combination with Survived was Passenger Class (-0.34) then Fare (0.26). "},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain_cor = train_raw[['Survived', 'Pclass', 'Age','SibSp','Parch','Fare']].copy()\ntrain_cor=dftrain_cor.corr()\n\nmask = np.zeros_like(train_cor)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(7, 5))\n    ax = sns.heatmap(train_cor, mask=mask, annot=True,linewidths=1, vmax=.3, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Bar Plots and Histograms**\n\nFollowing charts visualize various statistics of the data. The following are some conclusions made:\n- More females survived than males\n- Children (ages less than 10) had higher chance of survival\n- More people that boarded in Cherbourg survived\n- Large number of males in 3rd class"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='Survived',hue='Sex',data=train_raw)\nplt.ylabel('Frequency')\nplt.title(\"# of Survived\")\n\nplt.subplot2grid((1,2),(0,1))\nsns.countplot(x='Embarked',hue='Survived',data=train_raw)\nplt.ylabel(' ')\nplt.title(\"# of Embarked\")\nplt.show()\n\nplt.subplot2grid((1,2),(0,0))\nsns.countplot(x='Pclass',hue='Sex',data=train_raw)\nplt.ylabel('Frequency')\nplt.title(\"# of Each Sex\")\n\nplt.subplot2grid((1,2),(0,1))\nsns.countplot(x='Pclass',hue='Survived',data=train_raw)\nplt.ylabel(' ')\nplt.title(\"# of Survived\")\nplt.show()\n\nfig, ax = plt.subplots()\nsns.distplot(train_raw.Age[train_raw.Survived==1],kde=False,ax=ax, color=\"#1f77b4\")\nsns.distplot(train_raw.Age[train_raw.Survived==0],kde=False,ax=ax, color=\"#ff7f0e\")\nplt.title(\"Age Distribution\")\nplt.xlabel('Age')\nred_patch = mpatches.Patch(color='#1f77b4', label='Survived')\nblue_patch = mpatches.Patch(color='#ff7f0e', label='Died')\nplt.legend(handles=[red_patch, blue_patch] ,loc='best')\nplt.show()\n\nfig, ax = plt.subplots()\nsns.distplot(train_raw.Fare[train_raw.Pclass==1],kde=False,color=\"#1f77b4\", ax=ax)\nsns.distplot(train_raw.Fare[train_raw.Pclass==2],kde=False,color=\"#ff7f0e\", ax=ax)\nsns.distplot(train_raw.Fare[train_raw.Pclass==3],kde=False,color=\"#2ca02c\", ax=ax)\nplt.title(\"Fare Distribution\")\npatch_1 = mpatches.Patch(color='#1f77b4', label='Class 1')\npatch_2 = mpatches.Patch(color='#ff7f0e', label='Class 2')\npatch_3 = mpatches.Patch(color='#2ca02c', label='Class 3')\nplt.legend(handles=[patch_1, patch_2, patch_3])\nplt.xlabel('Fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pclass 1 Age Survival Figure\nfig, ax = plt.subplots()\nsns.distplot(train_raw.Age[train_raw.Survived==1][train_raw.Pclass==1],kde=False,ax=ax, color=\"#FFA500\")\nsns.distplot(train_raw.Age[train_raw.Survived==0][train_raw.Pclass==1],kde=False,ax=ax, color=\"#00FFFF\")\nred_patch = mpatches.Patch(color='#FFA500', label='Class 1 - Survived')\nblue_patch = mpatches.Patch(color='#00FFFF', label='Class 1 - Died')\nplt.legend(handles=[red_patch, blue_patch] ,loc='best')\n#Pclass 2 Age Survival Figure\nfig, ax = plt.subplots()\nsns.distplot(train_raw.Age[train_raw.Survived==1][train_raw.Pclass==2],kde=False,ax=ax, color=\"#FFA500\")\nsns.distplot(train_raw.Age[train_raw.Survived==0][train_raw.Pclass==2],kde=False,ax=ax, color=\"#00FFFF\")\nred_patch = mpatches.Patch(color='#FFA500', label='Class 2 - Survived')\nblue_patch = mpatches.Patch(color='#00FFFF', label='Class 2 - Died')\nplt.legend(handles=[red_patch, blue_patch] ,loc='best')\n#Pclass 3 Age Survival Figure\nfig, ax = plt.subplots()\nsns.distplot(train_raw.Age[train_raw.Survived==1][train_raw.Pclass==3],kde=False,ax=ax, color=\"#FFA500\")\nsns.distplot(train_raw.Age[train_raw.Survived==0][train_raw.Pclass==3],kde=False,ax=ax, color=\"#00FFFF\")\nred_patch = mpatches.Patch(color='#FFA500', label='Class 3 - Survived')\nblue_patch = mpatches.Patch(color='#00FFFF', label='Class 3 - Died')\nplt.legend(handles=[red_patch, blue_patch] ,loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filling in Missing Data <a id=\"Missing_data\"></a>\n\nIn both the train and test datasets are missing values in the age column. Since there are a lot of missing values filling in the data just based on general statistics, such as mean or mode, would be innacurate. Therefore should predict based on other columns. User Allohvk <sup>1</sup>   came up with the idea of using the title in the name to predict the age. Titles were grouped together since a few of them only had a few entries. Note that the title 'Fchild' relies on an engineered feature of a family size larger than 1. Since there were very few missing data in embarked and fare columns the median value was used. After missing features were put in one hot encoding was performed on all the non numerical values.\n\n</br> <sup>1</sup> The link to Allohvk's full notebook can be found here: https://www.kaggle.com/allohvk/titanic-missing-age-imputation-tutorial-advanced\n\n# Feature Engineering and Scaling <a id=\"feature_eng\"></a>\n\nVarious features were engineered. The following list outlines the different features and how they were calculated \n- Fam_size - Family size determined by adding number of siblings and spouse and number of parents and children plus one\n- Alone -  Binary feature determined on whether family size is equal to one \n- Ageclass - Product of age and class, idea based on user Manav Sehgal's notebook <sup>2</sup>  \n\nThe age and fare features also had boxcox transformation performed on them since these features were relatively skewed. Then finally a min-max scaler was applied to boxcox features and the classAge feature.\n \n</br> <sup>2</sup> The link to Manav Sehgal's full notebook can be found here: https://www.kaggle.com/startupsci/titanic-data-science-solutions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dictionary with all the titles\nTitleDict = {\"Capt\": \"Officer\",\"Col\": \"Officer\",\"Major\": \"Officer\",\"Jonkheer\": \"Royalty\", \\\n             \"Don\": \"Royalty\", \"Sir\" : \"Royalty\",\"Dr\": \"Royalty\",\"Rev\": \"Royalty\", \\\n             \"Countess\":\"Royalty\", \"Mme\": \"Mrs\", \"Mlle\": \"Miss\", \"Ms\": \"Mrs\",\"Mr\" : \"Mr\", \\\n             \"Mrs\" : \"Mrs\",\"Miss\" : \"Miss\",\"Master\" : \"Master\",\"Lady\" : \"Royalty\"}\n\nfor df in dfs:\n    df['Embarked']=df[['Embarked']].fillna(train_raw.mode()['Embarked'][0])\n    df['Fare']=df[['Fare']].fillna(train_raw['Fare'].median())\n    df['Title']=df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title']=df.Title.map(TitleDict)\n    df['Fam_size']=df['SibSp']+df['Parch']+1\n    df.loc[(df.Title=='Miss')&(df.Parch!=0)&(df.Fam_size>1),'Title']='Fchild'\n    df['Alone']=df['Fam_size'].apply(lambda x:1 if x==1 else 0)\n\nage_vals = dfs[0].groupby(['Pclass','Sex','Title'])['Age'].mean()\nfor df in dfs:\n    vals=df[df[\"Age\"].isnull()].index.values.astype(int).tolist()\n    for val in vals:\n        df.loc[val,'Age']=age_vals[df.loc[val,'Pclass'],df.loc[val,'Sex'],df.loc[val,'Title']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=dfs[0]\ntest=dfs[1]\ntrain=train.drop(['PassengerId','Name','Cabin','Survived','Ticket'],axis=1)\ntest=test.drop(['PassengerId','Name','Cabin','Ticket'],axis=1)\ntest['Title']=test[['Title']].fillna('Mrs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train)\ntest= pd.get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['boxAge']=stats.boxcox(train['Age'])[0]\ntrainfare=np.array(train['Fare'])\ntrainfare[trainfare<=0]=0.01\ntrain['boxFare']=stats.boxcox(trainfare)[0]\n\ntestfare=np.array(test['Fare'])\ntestfare[testfare<=0]=0.01\n\ntest['boxAge']=stats.boxcox(test['Age'], stats.boxcox(train['Age'])[1])\ntest['boxFare']=stats.boxcox(testfare, stats.boxcox(trainfare)[1])\ntrain=train.drop(['Age','Fare'], axis=1)\ntest=test.drop(['Age','Fare'], axis=1)\ntrain['Ageclass']=train['boxAge']*train['Pclass']\ntest['Ageclass']=test['boxAge']*train['Pclass']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_scaler = preprocessing.MinMaxScaler()\ntrain[['boxAge','boxFare','Ageclass']] = min_max_scaler.fit_transform(train[['boxAge','boxFare','Ageclass']])\ntest[['boxAge','boxFare','Ageclass']] = min_max_scaler.fit_transform(test[['boxAge','boxFare','Ageclass']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Models <a id=\"Ml_models\"></a>\n\nThree different machine learning models were used and compared. Namely linear regression, random forest and XGB. The cross validation test size was set to 0.2. The number of estimators for the Random Forest and the learning rate, subsample and column sample by tree variables for the XGB model were optimized through trial and error. A full summary of the results can be found in the conclusion. Predictions of each of the models were output as csv files."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression <a id=\"log_reg\"></a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"LogReg=LogisticRegression()\nLogReg.fit(X_train,y_train)\nlog_reg_s=LogReg.score(X_test,y_test)\nprint(log_reg_s)\nmodel_comp=pd.DataFrame({'ML Model':'Log_Reg','Score':[log_reg_s]})\npredict=LogReg.predict(test)\nresults=pd.DataFrame({'PassengerId':test_raw['PassengerId'],'Survived':pd.Series(predict)})\nresults.to_csv('resultslogreg.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest <a id=\"Rand_Forest\"></a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100,max_depth=5)\nmodel.fit(X_train, y_train)\nRand_For_S=model.score(X_test, y_test)\nmodel_comp=model_comp.append({'ML Model':'Rand_For','Score':Rand_For_S},ignore_index=True)\nprint(Rand_For_S)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model.predict(test)\nresults=pd.DataFrame({'PassengerId':test_raw['PassengerId'],'Survived':pd.Series(predict)})\nresults.to_csv('resultsrt.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGB <a id=\"XGB\"></a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nmy_modelxg = XGBClassifier(n_estimators=1000, learning_rate=0.1, subsample=0.9, colsample_bytree = 0.9)\nmy_modelxg.fit(X_train, y_train, \n             early_stopping_rounds=5,\n             eval_set=[(X_test, y_test)], \n             verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_cv = my_modelxg.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score\ncv_score=accuracy_score(y_test,predict_cv)\nmodel_comp=model_comp.append({'ML Model':'XGB','Score':cv_score},ignore_index=True)\nprint (\"CV Score: \",cv_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = my_modelxg.predict(test)\nresults=pd.DataFrame({'PassengerId':test_raw['PassengerId'],'Survived':pd.Series(predict)})\nresults.to_csv('resultsXG.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion <a id=\"conclusion\"></a> \n\nThe model with the highest cross validation score was the XGB model with 85.47% accuracy. However, after submitting all three solutions to the competition the random forest achieved the highest accuracy with 78.71% with XGB achieving 78.47% and the logistic regression achieving 77.75%\n\nThank you for visiting my kernel, any comments and suggestions are welcome!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_comp.sort_values(by=['Score'],ascending=False))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}